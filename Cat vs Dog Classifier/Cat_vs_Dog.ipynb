{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNY7x3af1ENMU2sKC6BP0Wg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JR-26/Classification-problems/blob/main/Cat%20vs%20Dog%20Classifier/Cat_vs_Dog.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GojMLatJYg1"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, Dense, Flatten, Dropout, GlobalMaxPooling2D, MaxPooling2D, BatchNormalization\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "R1Wzn8fmJboe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d salader/dogs-vs-cats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMFsiijZJeqM",
        "outputId": "65daaed1-556e-4126-aa18-db70fd0558da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Downloading dogs-vs-cats.zip to /content\n",
            " 99% 1.05G/1.06G [00:12<00:00, 206MB/s]\n",
            "100% 1.06G/1.06G [00:12<00:00, 94.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "jr=zipfile.ZipFile('/content/dogs-vs-cats.zip')\n",
        "jr.extractall('/content')\n",
        "jr.close()"
      ],
      "metadata": {
        "id": "bpGfYwrwJg-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jr='/content/dogs_vs_cats/test'"
      ],
      "metadata": {
        "id": "j6sFEGF-JkUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = {'cat': 0, 'dog': 1, 'cats': 0, 'dogs': 1, 'other': 2}\n",
        "def load_data():\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    size = (64, 64)\n",
        "\n",
        "    print('loading from data file', end=\"\")\n",
        "\n",
        "    for folder in os.listdir(jr):\n",
        "        fol = folder.strip('._')\n",
        "        path = os.path.join(jr, fol)\n",
        "        print(fol, end='|')\n",
        "\n",
        "        for image_name in os.listdir(path):\n",
        "            try:\n",
        "                temp_img = cv2.imread(os.path.join(path, image_name))\n",
        "                temp_img = cv2.resize(temp_img, size, interpolation=cv2.INTER_AREA)\n",
        "                images.append(temp_img)\n",
        "                labels.append(d[fol])\n",
        "                temp_img = cv2.flip(temp_img, flipCode=1)\n",
        "                images.append(temp_img)\n",
        "                labels.append(d[fol])\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing image {image_name}: {str(e)}\")\n",
        "\n",
        "    if not images or not labels:\n",
        "        print(\"Error: Images or labels list is empty.\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    images = np.array(images)\n",
        "    images = images.astype('float64') / 255.0\n",
        "    labels = np.array(labels)\n",
        "    labels = keras.utils.to_categorical(labels, num_classes=2)\n",
        "\n",
        "    print(\"Images shape:\", images.shape)\n",
        "    print(\"Labels shape:\", labels.shape)\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.2)\n",
        "\n",
        "    print(\"x_train shape:\", x_train.shape)\n",
        "    print(\"x_test shape:\", x_test.shape)\n",
        "    print(\"y_train shape:\", y_train.shape)\n",
        "    print(\"y_test shape:\", y_test.shape)\n",
        "\n",
        "    return x_train, x_test, y_train, y_test\n",
        "\n",
        "# Replace the existing load_data function with the modified one\n",
        "x_train, x_test, y_train, y_test = load_data()\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "i = Input(shape=x_train[0].shape)\n",
        "x = Conv2D(32, (3, 3), activation=\"relu\", padding='same')(i)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(32, (3, 3), activation=\"relu\", padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D(2, 2)(x)\n",
        "\n",
        "x = Conv2D(64, (3, 3), activation=\"relu\", padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(64, (3, 3), activation=\"relu\", padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D(2, 2)(x)\n",
        "\n",
        "x = Conv2D(128, (3, 3), activation=\"relu\", padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(128, (3, 3), activation=\"relu\", padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D(2, 2)(x)\n",
        "\n",
        "x = Flatten()(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "num_classes = 2\n",
        "x = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "model = Model(i, x)\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "batch_size = 32\n",
        "steps_per_epoch = len(x_train) // batch_size if len(x_train) % batch_size == 0 else len(x_train) // batch_size + 1\n",
        "\n",
        "r = model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "              validation_data=(x_test, y_test),\n",
        "              steps_per_epoch=steps_per_epoch,\n",
        "              epochs=15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrUC8z4VJx1z",
        "outputId": "d1220e09-312a-4f01-b8e8-399bedb72ce5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading from data filedogs|cats|Images shape: (10000, 64, 64, 3)\n",
            "Labels shape: (10000, 2)\n",
            "x_train shape: (8000, 64, 64, 3)\n",
            "x_test shape: (2000, 64, 64, 3)\n",
            "y_train shape: (8000, 2)\n",
            "y_test shape: (2000, 2)\n",
            "Epoch 1/15\n",
            "250/250 [==============================] - 291s 1s/step - loss: 1.0425 - accuracy: 0.6136 - val_loss: 0.7497 - val_accuracy: 0.5035\n",
            "Epoch 2/15\n",
            "250/250 [==============================] - 288s 1s/step - loss: 0.6070 - accuracy: 0.6737 - val_loss: 0.5828 - val_accuracy: 0.7130\n",
            "Epoch 3/15\n",
            "250/250 [==============================] - 290s 1s/step - loss: 0.6029 - accuracy: 0.6773 - val_loss: 0.6066 - val_accuracy: 0.6635\n",
            "Epoch 4/15\n",
            "250/250 [==============================] - 301s 1s/step - loss: 0.5684 - accuracy: 0.7048 - val_loss: 0.6192 - val_accuracy: 0.6670\n",
            "Epoch 5/15\n",
            "250/250 [==============================] - 276s 1s/step - loss: 0.5555 - accuracy: 0.7197 - val_loss: 0.5679 - val_accuracy: 0.7095\n",
            "Epoch 6/15\n",
            "250/250 [==============================] - 281s 1s/step - loss: 0.5080 - accuracy: 0.7519 - val_loss: 0.5429 - val_accuracy: 0.7325\n",
            "Epoch 7/15\n",
            "250/250 [==============================] - 278s 1s/step - loss: 0.5174 - accuracy: 0.7494 - val_loss: 0.5276 - val_accuracy: 0.7370\n",
            "Epoch 8/15\n",
            "250/250 [==============================] - 305s 1s/step - loss: 0.4754 - accuracy: 0.7751 - val_loss: 0.4391 - val_accuracy: 0.7985\n",
            "Epoch 9/15\n",
            "250/250 [==============================] - 278s 1s/step - loss: 0.4469 - accuracy: 0.7985 - val_loss: 0.4432 - val_accuracy: 0.7935\n",
            "Epoch 10/15\n",
            "250/250 [==============================] - 303s 1s/step - loss: 0.4313 - accuracy: 0.8069 - val_loss: 0.5681 - val_accuracy: 0.7430\n",
            "Epoch 11/15\n",
            "250/250 [==============================] - 295s 1s/step - loss: 0.3966 - accuracy: 0.8181 - val_loss: 0.4812 - val_accuracy: 0.7710\n",
            "Epoch 12/15\n",
            "250/250 [==============================] - 293s 1s/step - loss: 0.3914 - accuracy: 0.8276 - val_loss: 0.6068 - val_accuracy: 0.7365\n",
            "Epoch 13/15\n",
            "250/250 [==============================] - 287s 1s/step - loss: 0.3426 - accuracy: 0.8515 - val_loss: 0.3630 - val_accuracy: 0.8460\n",
            "Epoch 14/15\n",
            "250/250 [==============================] - 293s 1s/step - loss: 0.3494 - accuracy: 0.8508 - val_loss: 0.4294 - val_accuracy: 0.8060\n",
            "Epoch 15/15\n",
            "250/250 [==============================] - 299s 1s/step - loss: 0.3382 - accuracy: 0.8611 - val_loss: 0.2869 - val_accuracy: 0.8790\n"
          ]
        }
      ]
    }
  ]
}